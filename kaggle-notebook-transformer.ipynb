{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Libraries \n# Import the important libraries\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport math\n# from transformer_model import build_transformer\n# from datasets import load_dataset #hugging face datasets\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\n\n# Huggingface datasets and tokenizers\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T15:26:49.094981Z","iopub.execute_input":"2025-03-03T15:26:49.095247Z","iopub.status.idle":"2025-03-03T15:26:55.054068Z","shell.execute_reply.started":"2025-03-03T15:26:49.095210Z","shell.execute_reply":"2025-03-03T15:26:55.053075Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class InputEmbedding(nn.Module):\n\n    ''' This class is used to create the input embedding layer for the transformer model\n         which is used to convert the input tokens into the embedding vectors of the same size as the model dimension \n    Args:\n        d_model: int: the dimension of the model (default= 512) also known as the embedding size\n        vocab_size: int: the size of the vocabulary (depends on the dataset)\n        padding_idx: int: the padding index for the padding token (default= 0)\n\n    Returns:\n        input_embedding: tensor: the input embedding layer for the transformer model\n\n    Note: currently we are not using the padding index, but we can use it in the future for that we need to pass the padding index as an argument\n    '''\n    def __init__(self, d_model: int, vocab_size : int):\n        super(InputEmbedding, self).__init__()\n        self.d_model = d_model\n        self.vocab_size = vocab_size    \n        self.embedding = nn.Embedding(vocab_size, d_model) # creating the embedding layer using the nn.Embedding class \n        '''\n        nn.Embedding: A simple lookup table that stores embeddings of a fixed dictionary and size.\n        This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.\n        '''\n\n    def forward(self, x):\n        '''\n        This function is used to pass the input tokens through the embedding layer to get the embedding vectors\n        Args:\n            x: tensor: the input tokens of shape (batch_size, seq_len)\n            batch_size: int: the size of the batch\n            seq_len: int: the length of each input sequence\n        Returns:\n            embedding: tensor: the embedding vectors of shape (batch_size, seq_len, d_model)\n            multiplying the embedding vectors with the square root of the d_model is prevent the gradients from vanishing or exploding\n        '''\n        return self.embedding(x)*math.sqrt(self.d_model) \n\n# 2. Positional Encoding\n\nclass PositionalEmbedding(nn.Module):\n    \n    ''' Transformers process input tokens in parallel and lack order information. \n        To address this, a positional encoding layer adds positional information using a positional encoding matrix, ensuring the model understands token order.\n    Args:\n        d_model: int: the dimension of the model (default= 512) also known as the embedding size\n        max_len: int: the maximum length of the input sequence (default= 512)\n\n    Returns:\n        positional_encoding: tensor: the positional encoding layer for the transformer model\n\n    Note: the positional encoding is added to the input embedding vectors to add the positional information to the input tokens\n    '''\n    def __init__(self, d_model: int, max_seq_len: int, dropout: float):\n        super(PositionalEmbedding, self).__init__()\n        self.d_model = d_model\n        self.max_seq_len = max_seq_len\n        self.dropout = nn.Dropout(dropout) # adding the dropout layer to the positional encoding to prevent overfitting\n        # self.positional_encoding = self.get_positional_encoding() # getting the positional encoding matrix\n\n        pos_emb = torch.zeros(max_seq_len, d_model)\n        position = torch.arange(0, max_seq_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float()*(-math.log(10000.0)/d_model))\n        pos_emb[:, 0::2] = torch.sin(position*div_term)\n        pos_emb[:, 1::2] = torch.cos(position*div_term)\n\n        self.register_buffer('pos_emb', pos_emb.unsqueeze(0))\n\n    # def get_positional_encoding(self):\n    #     '''\n    #     This function is used to create the positional encoding matrix\n    #     Returns:\n    #         positional_encoding: tensor: the positional encoding matrix of shape (max_len, d_model)\n    #     '''\n    #     positional_encoding = torch.zeros(self.max_len, self.d_model) # creating a matrix of zeros of shape (max_len, d_model)\n\n    #     position = torch.arange(0, self.max_len, dtype= torch.float).unsqueeze(1) # creating a position matrix of shape (max_len, 1) \n    #     div_term = torch.exp(torch.arange(0, self.d_model, 2).float()*(-math.log(10000.0)/self.d_model)) # creating a division term\n\n    #     positional_encoding[:, 0::2] = np.sin(position*div_term) # adding the sin values to the even indices\n    #     positional_encoding[:, 1::2] = np.cos(position*div_term) # adding the cos values to the odd indices\n\n    #     positional_encoding = positional_encoding.unsqueeze(0)\n    #     return positional_encoding\n\n    def forward(self, x):\n        '''\n        This function is used to add the positional encoding to the input embedding vectors\n        Args:\n            x: tensor: the input embedding vectors of shape (batch_size, seq_len, d_model)\n            batch_size: int: the size of the batch\n            seq_len: int: the length of each input sequence\n        Returns:\n            x + positional_encoding: tensor: the input embedding vectors with the positional encoding added of shape (batch_size, seq_len, d_model)\n        '''\n        x = x + self.pos_emb[:, :x.size(1), :].requires_grad_(False) #requires_grad_(False) is used to prevent the positional encoding matrix from being updated during the training\n        x = self.dropout(x)\n        return x\n\n# 3. Layer Normalization\n\n# class LayerNormalization(nn.Module):\n#     ''' This class is used to create the layer normalization layer for the transformer model\n#     Args:\n#         eps: float: a value added to the denominator for numerical stability (default= 1e-6)\n#             so that the layer normalization does not divide by zero\n\n#     Returns:\n#         layer_norm: tensor: the layer normalization layer for the transformer model\n#     '''\n#     def __init__(self,  eps: float = 1e-6) -> None:\n#         super(LayerNormalization, self).__init__()\n#         self.eps = eps\n#         self.alpha = nn.Parameter(torch.ones(1)) # creating a learnable parameter alpha: multiplicative\n#         self.bias = nn.Parameter(torch.zeros(1)) # creating a learnable parameter bias: additive\n\n#     def forward(self, x):\n#         '''\n#         This function is used to normalize the input embedding vectors\n#         Args:\n#             X: tensor: the input embedding vectors of shape (batch_size, seq_len, d_model)\n#         returns:\n#             layer_norm: tensor: normalized vectors (x*alpha + bias) of shape (batch_size, seq_len, d_model)\n\n#         '''\n#         mean = x.mean(dim = -1, keepdim=True) # dim = -1 is used to calculate the mean along the last dimension\n#         std = x.std(dim = -1, keepdim=True)   \n#         layer_norm = self.alpha*(x - mean)/(std + self.eps) + self.bias\n#         return layer_norm\n\n# 4. Feed Forward Network\n\nclass FFN(nn.Module):\n    '''\n    This class is used to create the feed forward network for the transformer model\n    used the ReLU activation function and two linear layers\n    Args:\n        d_model: int: the dimension of the model (default= 512) also known as the embedding size\n        d_ff: int: the dimension of the feed forward network (default= 2048)\n        dropout: float: the dropout rate (default= 0)\n            \n    Returns:\n        ffn: tensor: the feed forward network for the transformer model\n        \n    '''\n    def __init__(self, d_model: int , d_ff : int, dropout: float):\n        super(FFN, self).__init__()\n        self.d_model = d_model\n        self.d_ff = d_ff\n        self.dropout = nn.Dropout(dropout)\n        self.fc1 = nn.Linear(self.d_model, self.d_ff) # creating the first fully connected linear layer \n        self.fc2 = nn.Linear(self.d_ff, self.d_model) # creating the second fully connected linear layer\n        self.relu = nn.ReLU() # creating the ReLU activation function\n\n    def forward(self, x):\n        '''\n        This function is used to pass the input embedding vectors through the feed forward network\n        Args:\n            x: tensor: the input embedding vectors of shape (batch_size, seq_len, d_model)\n            batch_size: int: the size of the batch\n            seq_len: int: the length of each input sequence\n            d_model: int: the dimension of the model (default= 512) also known as the embedding size\n        Returns:\n            ffn: tensor: the output of the feed forward network of shape (batch_size, seq_len, d_model)\n        '''\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\n\n\n# 5. Multi-Head Attention\n\nclass MultiHeadAttention(nn.Module):\n    '''\n    - This class is used to create the multi-head attention layer for the transformer model\n    - used to calculate the attention scores between the input embedding vectors\n    Args:\n        d_model: int: the dimension of the model (default= 512) also known as the embedding size\n        num_heads: int: the number of attention heads (default= 4)\n        dropout: float: the dropout rate (default= 0)\n    Returns:\n        multihead_attention: tensor: the multi-head attention layer for the transformer model\n    \n    '''\n    def __init__(self, d_model: int, num_heads: int, dropout: float):\n        super(MultiHeadAttention, self).__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.dropout = nn.Dropout(dropout)\n        # to make sure the d_model is divisible by the number of heads\n        assert d_model % num_heads == 0\n\n        self.d_k = d_model // self.num_heads # d_k is the dimension of the key and value vectors\n        '''\n            - d_k is the dimension of the key and value vectors\n            - w_q is the weight matrix for the query vectors of shape (d_model, d_model)\n            - w_k is the weight matrix for the key vectors of shape (d_model, d_model)\n            - w_v is the weight matrix for the value vectors of shape (d_model, d_model)\n            - w_o is the weight matrix for the output vectors of shape (d_model, d_model)\n            nn.Linear is used to create a linear layer and it applies a linear transformation y = xW^T + b\n        '''\n        self.w_q = nn.Linear(self.d_model, self.d_model, bias = False)\n        self.w_k = nn.Linear(self.d_model, self.d_model, bias = False)\n        self.w_v = nn.Linear(self.d_model, self.d_model, bias = False)\n        self.w_o = nn.Linear(self.d_model, self.d_model, bias= False) #d_model = d_k*num_heads and d_k == d_v\n\n    @staticmethod\n    def self_attention(q, k, v, mask, dropout: nn.Dropout):\n        '''\n        shape of the query is (batch_size, num_heads, seq_len, d_k)\n        extract the d_k dimension from the query tensor\n        '''\n        d_k = q.size(-1)\n\n        # calculating the attention scores\n        attn_score = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k) # calculating the dot product of the query and key vectors\n        if mask is not None:\n            attn_score = attn_score.masked_fill(mask == 0, -1e9)\n        attn_score = attn_score.softmax(dim = -1)\n\n        if dropout is not None:\n            attn_score = dropout(attn_score)\n\n        output = torch.matmul(attn_score, v) # calculating the dot product of the attention scores and value vectors\n        return output, attn_score\n\n\n    def forward(self, q, k, v, mask):\n        '''\n        self.w_q(q) is used to calculate the query vectors or applies the linear transformation to the query vectors\n        query = q @ w_q^T + b_q   \n        q: tensor: the query vectors of shape (batch_size, seq_len, d_model)\n        w_q: tensor: the weight matrix for the query vectors of shape (d_model, d_model)\n        b_q: tensor: the bias for the query vectors of shape (d_model)\n        query: tensor: the query vectors after the linear transformation of shape (batch_size, seq_len, d_model)\n            \n        '''\n        Q = self.w_q(q)\n        K = self.w_k(k)\n        V = self.w_v(v)\n\n        #  split the d_model into num_heads\n        batch_size = Q.size(0)\n        max_seq_len = Q.size(1)\n        \n\n        '''\n        - splitting the query, key, and value vectors into a number of heads so that we can calculate the attention scores in parallel\n        - splitingn the heads allows the model to focus on different aspects of the input sequence\n        - view is used to change the shape of the tensor to (batch_size, seq_len, num_heads, d_k)\n        - permute is used to change the dimensions of the tensor to (batch_size, num_heads, seq_len, d_k) : basically it's changing the position dimensions\n        '''\n        Q = Q.view(batch_size, max_seq_len, self.num_heads, self.d_k).transpose(1,2)\n        K = K.view(batch_size, max_seq_len, self.num_heads, self.d_k).transpose(1,2)\n        V = V.view(batch_size, max_seq_len, self.num_heads, self.d_k).transpose(1,2) \n\n        attn_output, attnt_score = MultiHeadAttention.self_attention(Q, K, V, mask, self.dropout)\n\n        # concatenating the heads\n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, max_seq_len, self.d_model)\n        # apply the final linear layer\n        attn_output = self.w_o(attn_output)\n        return attn_output\n\n       \n        \n# 6. Residual Connection\n\nclass ResidualConnection(nn.Module):\n    '''\n        It's also know as skip connection\n        This class is used to create the residual connection for the transformer model\n\n        Args:\n            d_model: int: the dimension of the model (default= 512) also known as the embedding size\n            dropout: float: the dropout rate (default= 0)\n            sublayer: nn.Module: the sublayers (Multihead self-attention layer or ffn) to be added to the residual connection\n\n        Returns:\n            residual_connection: tensor: the residual connection for the transformer model\n    '''\n    def __init__(self, features: int, dropout: float) -> None:\n        super(ResidualConnection, self).__init__()\n        \n        self.dropout = nn.Dropout(dropout)\n        self.norm = nn.LayerNorm(features)\n\n    def forward(self, x, sublayer):\n        '''\n        This function is used to add the sublayer to the residual connection\n        Args:\n            x: tensor: the input embedding vectors of shape (batch_size, seq_len, d_model)\n            sublayer: nn.Module: the sublayers (Multihead self-attention layer or ffn) to be added to the residual connection\n        Returns:\n            residual_connection: tensor: the output of the residual connection of shape (batch_size, seq_len, d_model)\n        '''\n        x = self.norm(x)\n        x = x + self.dropout(sublayer(x))\n        return x\n\n# Encoder Block\nclass EncoderBlock(nn.Module):\n    '''\n    Encoder block consists of:\n    - Multihead self-attention layer\n    - Residual connection\n    - Feed forward network\n    - Residual connection\n    Args:\n        self_attention: nn.Module: the multihead self-attention layer\n        ffn: nn.Module: the feed forward network\n        dropout: float: the dropout rate (default= 0)\n    Returns:\n        encoder_block: tensor: the encoder block for the transformer model\n    '''        \n    def __init__(self, features: int, self_attn: MultiHeadAttention,feed_forward: FFN, dropout: float):\n        super(EncoderBlock, self).__init__()\n        self.features = features\n        self.self_attention = self_attn\n        self.feed_forward_network = feed_forward\n        self.residual_connection_1 = ResidualConnection(features, dropout)\n        self.residual_connection_2 = ResidualConnection(features, dropout)\n\n\n\n    def forward(self, x, mask):\n        '''\n        args:\n            x: tensor: the input embedding vectors of shape (batch_size, seq_len, d_model)\n            mask: tensor: the mask to be applied to the input embedding vectors of shape (batch_size, seq_len, seq_len)\n        Returns:\n            encoder_block: tensor: the output of the encoder block of shape (batch_size, seq_len, d_model)\n        '''\n        x = self.residual_connection_1(x, lambda x: self.self_attention(x, x, x, mask))\n        x = self.residual_connection_2(x, self.feed_forward_network)\n        return x\n\n\n# Encoder consists of N encoder blocks \nclass Encoder(nn.Module):\n    '''\n    Encoder consists of N encoder blocks\n    Args:\n        layers or encoder_blocks: nn.ModuleList: the list of encoder blocks\n    Returns:\n        encoder: tensor: the encoder for the transformer model\n    '''\n    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n        super(Encoder, self).__init__()\n        self.features = features\n        self.layers = layers\n        self.norm = nn.LayerNorm(features)\n\n    def forward(self, x, source_mask):\n        '''\n        args:\n            x: tensor: the input embedding vectors of shape (batch_size, seq_len, d_model)\n            mask: tensor: the mask to be applied to the input embedding vectors of shape (batch_size, seq_len, seq_len)\n        Returns:\n            encoder: tensor: the output of the encoder of shape (batch_size, seq_len, d_model)\n        '''\n        for encoder_block in self.layers: # iterating through the encoder blocks (layers)\n            x = encoder_block(x, source_mask)\n        return self.norm(x)\n\n# Decoder Block\nclass DecoderBlock(nn.Module):\n    '''\n    Decoder block consists of:\n    - Multihead self-attention layer\n    - Residual connection\n    - Multihead masked self-attention layer\n    - Residual connection\n    - Feed forward network\n    - Residual connection\n    Args:\n        self_attention: MultiheadAttention: the multihead self-attention layer\n        cross_attention: MultiheadAttention: the multihead masked self-attention layer\n        ffn: nn.Module: the feed forward network\n        dropout: float: the dropout rate (default= 0)\n    Returns:\n        decoder_block: tensor: the decoder block for the transformer model\n    '''\n    def __init__(self, features: int, self_attn: MultiHeadAttention, cross_attn: MultiHeadAttention, feed_forward: FFN, dropout: float):\n        super(DecoderBlock, self).__init__()\n        self.features = features\n        self.self_attn = self_attn\n        self.cross_attn = cross_attn\n        self.feed_forward = feed_forward\n        self.dropout = nn.Dropout(dropout)\n\n        self.residual_connection1 = ResidualConnection(features, dropout)\n        self.residual_connection2 = ResidualConnection(features, dropout)\n        self.residual_connection3 = ResidualConnection(features, dropout)\n \n\n\n    def forward(self, x, enc_out, src_mask, tgt_mask):\n        '''\n        args:\n            x: tensor: the input embedding vectors of shape (batch_size, seq_len, d_model)\n            encoder_output: tensor: the output of the encoder of shape (batch_size, seq_len, d_model)\n            src_mask: tensor: the mask to be applied to the input embedding vectors of shape (batch_size, seq_len, seq_len)\n            tgt_mask: tensor: the mask to be applied to the input embedding vectors of shape (batch_size, seq_len, seq_len)\n        Returns:\n            decoder_block: tensor: the output of the decoder block of shape (batch_size, seq_len, d_model)\n        '''\n        x = self.residual_connection1(x, lambda x: self.self_attn(x, x, x, tgt_mask))\n        x = self.residual_connection2(x, lambda x: self.cross_attn(x, enc_out, enc_out, src_mask))\n        x = self.residual_connection3(x, self.feed_forward)\n        return x\n\n\n# Decoder consists of N decoder blocks\nclass Decoder(nn.Module):\n    '''\n    Decoder consists of N decoder blocks\n    Args:\n        layers or decoder_blocks: nn.ModuleList: the list of decoder blocks\n    Returns:\n        decoder: tensor: the decoder for the transformer model\n    '''\n    def __init__(self, features: int, layers: nn.ModuleList):\n        super(Decoder, self).__init__()\n        self.features = features\n        self.decoder_blocks = layers\n        self.norm = nn.LayerNorm(features)\n\n    def forward(self, x, enc_out, src_mask, tgt_mask):\n        '''\n        args:\n            x: tensor: the input embedding vectors of shape (batch_size, seq_len, d_model)\n            encoder_output: tensor: the output of the encoder of shape (batch_size, seq_len, d_model)\n            src_mask: tensor: the mask to be applied to the input embedding vectors of shape (batch_size, seq_len, seq_len)\n            tgt_mask: tensor: the mask to be applied to the input embedding vectors of shape (batch_size, seq_len, seq_len)\n        Returns:\n            decoder: tensor: the output of the decoder of shape (batch_size, seq_len, d_model)\n        '''\n        for decoder_block in self.decoder_blocks: # iterating through the decoder blocks (layers)\n            x = decoder_block(x, enc_out, src_mask, tgt_mask)\n        return self.norm(x)\n          \n\n# Linear Layer\nclass LinearLayer(nn.Module):\n    '''\n    Linear layer is used to convert the output of the transformer model to the output vocabulary size\n    Args:\n        d_model: int: the dimension of the model (default= 512) also known as the embedding size\n        vocab_size: int: the size of the vocabulary (depends on the dataset)\n    Returns:\n        linear_layer: tensor: the linear layer for the transformer model\n    '''\n    def __init__(self, d_model: int, vocab_size: int) -> None:\n        super(LinearLayer, self).__init__()\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.linear = nn.Linear(d_model, vocab_size)\n\n    def forward(self, x):\n        '''\n        args:\n            x: tensor: the input embedding vectors of shape (batch_size, seq_len, d_model)\n        Returns:\n            linear_layer: tensor: the output of the linear layer of shape (batch_size, seq_len, vocab_size)\n            also apply the softmax function to the output to get the probabilities of the output tokens\n        '''\n        return self.linear(x)\n\n\n# Now connect all the components to create the transformer model\nclass Transformer(nn.Module):\n    '''\n    This class is used to create the transformer model\n    Args:\n        src_embedding: InputEmbedding : the input embedding layer\n        trg_embedding: InputEmbedding: the target embedding layer\n        src_postional_encoding: PositionalEncoding: the positional encoding layer for the source\n        trg_postional_encoding: PositionalEncoding: the positional encoding layer for the target\n        encoder: Encoder: the encoder\n        decoder: Decoder: the decoder\n        linear_layer: LinearLayer: the linear layer\n    Returns:\n        transformer: tensor: the transformer model\n    '''   \n    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbedding, tgt_embed: InputEmbedding, src_pos_embed: PositionalEmbedding, tgt_pos_embed: PositionalEmbedding, linearlayer: LinearLayer) -> None:\n        super(Transformer, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embedding = src_embed\n        self.trg_embedding = tgt_embed\n        self.src_postional_encoding = src_pos_embed\n        self.trg_postional_encoding = tgt_pos_embed\n        self.linear = linearlayer\n\n\n    def encode(self, src, src_mask):\n        '''\n        args:\n            src: tensor: the input tokens of shape (batch_size, seq_len)\n            src_mask: tensor: the mask to be applied to the input embedding vectors of shape (batch_size, seq_len, seq_len)\n        Returns:\n            encoder_output: tensor: the output of the encoder of shape (batch_size, seq_len, d_model)\n        '''\n\n        src = self.src_embedding(src)\n        src = self.src_postional_encoding(src)\n        return self.encoder(src, src_mask)\n    \n    def decode(self, target, enc_out, src_mask, tgt_mask):\n        '''\n        args:\n            trg: tensor: the target tokens of shape (batch_size, seq_len)\n            encoder_output: tensor: the output of the encoder of shape (batch_size, seq_len, d_model)\n            src_mask: tensor: the mask to be applied to the input embedding vectors of shape (batch_size, seq_len, seq_len)\n            tgt_mask: tensor: the mask to be applied to the input embedding vectors of shape (batch_size, seq_len, seq_len)\n        Returns:\n            decoder_output: tensor: the output of the decoder of shape (batch_size, seq_len, d_model)\n        '''\n        target = self.trg_embedding(target)\n        target = self.trg_postional_encoding(target)\n        return self.decoder(target, enc_out, src_mask, tgt_mask)\n    \n\n    def linearlayer(self, x):\n        '''\n        args:\n            x: tensor: the input embedding vectors of shape (batch_size, seq_len, d_model)\n        Returns:\n            linear_layer: tensor: the output of the linear layer of shape (batch_size, seq_len, vocab_size)\n        '''\n        return self.linear(x)\n\n\n# building the transformer model\ndef trans_model(src_vocab, tgt_vocab, src_seq_len, tgt_seq_len, d_model:int = 512, num_heads: int = 8, d_ff: int = 2048, num_layers: int = 6, dropout: float = 0.1):\n    '''\n    This function is used to build the transformer model\n    Args:\n        src_vocab: int: the size of the source vocabulary\n        tgt_vocab: int: the size of the target vocabulary\n        src_seq_len: int: the length of the source sequence\n        tgt_seq_len: int: the length of the target sequence\n        d_model: int: the dimension of the model (default= 512) also known as the embedding size\n        num_heads: int: the number of attention heads (default= 8)\n        d_ff: int: the dimension of the feed forward network (default= 2048)\n        num_layers: int: the number of encoder and decoder blocks (default= 6)\n        dropout: float: the dropout rate (default= 0.1)\n    '''\n    # creating the input embedding layer for the source and target\n    source_embedding = InputEmbedding(d_model, src_vocab)\n    target_embedding = InputEmbedding(d_model, tgt_vocab)\n    src_pos_embed = PositionalEmbedding(d_model, src_seq_len, dropout)\n    tgt_pos_embed = PositionalEmbedding(d_model, tgt_seq_len, dropout)\n\n    # creating the encoder and decoder blocks\n    encoder_blocks = []\n    for i in range(num_layers):\n        enc_self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        ffn = FFN(d_model, d_ff, dropout)\n        encoder_block = EncoderBlock(d_model, enc_self_attn, ffn, dropout)\n        encoder_blocks.append(encoder_block)\n\n    # creating decoder blocks\n    decoder_blocks = []\n    for i in range(num_layers):\n        dec_self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        dec_cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        dec_ffn = FFN(d_model, d_ff, dropout)\n        dec_block = DecoderBlock(d_model, dec_self_attn, dec_cross_attn, dec_ffn, dropout)\n        decoder_blocks.append(dec_block)\n\n    # creating the encoder and decoder\n    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n\n    # creating the linear layer\n    linearlayer = LinearLayer(d_model, tgt_vocab)\n    # create the transformer model\n    transformer = Transformer(encoder, decoder, source_embedding, target_embedding, src_pos_embed, tgt_pos_embed, linearlayer)\n\n    for parameter in transformer.parameters():\n        if parameter.dim() > 1:\n            nn.init.xavier_uniform_(parameter)\n\n    return transformer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T15:26:55.055583Z","iopub.execute_input":"2025-03-03T15:26:55.056156Z","iopub.status.idle":"2025-03-03T15:26:55.089081Z","shell.execute_reply.started":"2025-03-03T15:26:55.056123Z","shell.execute_reply":"2025-03-03T15:26:55.088028Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def cml(size):\n    masked = torch.triu(torch.ones(size, size), diagonal=1).type(torch.int64)\n    return masked","metadata":{"execution":{"iopub.status.busy":"2025-03-03T15:26:55.090446Z","iopub.execute_input":"2025-03-03T15:26:55.090837Z","iopub.status.idle":"2025-03-03T15:26:55.113646Z","shell.execute_reply.started":"2025-03-03T15:26:55.090811Z","shell.execute_reply":"2025-03-03T15:26:55.112746Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, data, sorce_leng, target_leng, src_tokenizer, tgt_tokenizer, max_seq_len):\n        # super(CustomDataset, self).__init__()\n        self.data = data\n        self.src_leng = sorce_leng\n        self.tgt_leng = target_leng\n        self.src_tokenizer = src_tokenizer\n        self.tgt_tokenizer = tgt_tokenizer\n        self.max_seq_len = max_seq_len\n\n        # create the token_ids for special tokens\n        self.start_token_id = torch.tensor([tgt_tokenizer.token_to_id('[SOS]')], dtype=torch.int64)\n        self.end_token_id = torch.tensor([tgt_tokenizer.token_to_id('[EOS]')], dtype = torch.int64)\n        self.pad_token_id = torch.tensor([tgt_tokenizer.token_to_id('[PAD]')], dtype = torch.int64)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sen = self.data[idx]\n        src_sen = sen[self.src_leng]\n        tgt_sen = sen[self.tgt_leng]\n\n        # tokenize the source sentence\n        src_token = self.src_tokenizer.encode(src_sen).ids\n        # tokenize the target sentence\n        tgt_token = self.tgt_tokenizer.encode(tgt_sen).ids\n\n        # Require number of padding tokens\n        num_src_pad = self.max_seq_len - len(src_token) - 2\n        num_tgt_pad = self.max_seq_len - len(tgt_token) - 1\n\n        # condition for padding\n        if num_src_pad < 0 or num_tgt_pad < 0:\n            raise ValueError('The sentence is too long')\n        \n        # add the special tokens\n        encoder_input = torch.cat(\n            [\n                self.start_token_id,\n                torch.tensor(src_token, dtype=torch.int64),\n                self.end_token_id,\n                self.pad_token_id.repeat(num_src_pad)\n            ], dim = 0\n        )\n        decoder_input = torch.cat(\n            [\n                self.start_token_id,\n                torch.tensor(tgt_token, dtype=torch.int64),\n                self.pad_token_id.repeat(num_tgt_pad)\n            ], dim = 0\n        )\n\n        # create the target tensor\n        target = torch.cat(\n            [\n                torch.tensor(tgt_token, dtype=torch.int64),\n                self.end_token_id,\n                self.pad_token_id.repeat(num_tgt_pad)\n            ], dim = 0\n        )\n\n        out = {\n            \"encoder_input\": encoder_input,\n            \"decoder_input\": decoder_input,\n            \"target\": target,\n            \"encoder_mask\" : (encoder_input != self.pad_token_id).unsqueeze(0).unsqueeze(0),\n            \"decoder_mask\" : (decoder_input != self.pad_token_id).unsqueeze(0).unsqueeze(0) & cml(decoder_input.size(0)),\n            \"src_sen\": src_sen,\n            \"tgt_sen\": tgt_sen\n        }\n\n        return out\n            ","metadata":{"execution":{"iopub.status.busy":"2025-03-03T15:26:55.114354Z","iopub.execute_input":"2025-03-03T15:26:55.114610Z","iopub.status.idle":"2025-03-03T15:26:55.126667Z","shell.execute_reply.started":"2025-03-03T15:26:55.114586Z","shell.execute_reply":"2025-03-03T15:26:55.125945Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def get_sentences(data, leng):\n    for sen in data:\n        yield sen[leng]","metadata":{"execution":{"iopub.status.busy":"2025-03-03T15:26:55.128444Z","iopub.execute_input":"2025-03-03T15:26:55.128749Z","iopub.status.idle":"2025-03-03T15:26:55.145042Z","shell.execute_reply.started":"2025-03-03T15:26:55.128699Z","shell.execute_reply":"2025-03-03T15:26:55.144235Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def word_tokenizer(data, leng):\n    tokenizer = Tokenizer(WordLevel(unk_token = '[UNK]'))\n    tokenizer.pre_tokenizer = Whitespace()\n    trainer = WordLevelTrainer(special_tokens=['[PAD]', '[SOS]', '[EOS]', '[UNK]'])\n    tokenizer.train_from_iterator(get_sentences(data, leng), trainer= trainer)\n    return tokenizer","metadata":{"execution":{"iopub.status.busy":"2025-03-03T15:26:55.146299Z","iopub.execute_input":"2025-03-03T15:26:55.146604Z","iopub.status.idle":"2025-03-03T15:26:55.160458Z","shell.execute_reply.started":"2025-03-03T15:26:55.146582Z","shell.execute_reply":"2025-03-03T15:26:55.159680Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"eng_hin_dataset = load_dataset(\"cfilt/iitb-english-hindi\", split = 'train')\n\neng_hin_dataset = eng_hin_dataset['translation']","metadata":{"execution":{"iopub.status.busy":"2025-03-03T15:26:55.161300Z","iopub.execute_input":"2025-03-03T15:26:55.161615Z","iopub.status.idle":"2025-03-03T15:27:10.423305Z","shell.execute_reply.started":"2025-03-03T15:26:55.161576Z","shell.execute_reply":"2025-03-03T15:27:10.422305Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/3.14k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baea256b0a3743928cd4135baae75910"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dataset_infos.json:   0%|          | 0.00/953 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db264780cbe646fc86d8cadec9056117"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/190M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66371cc3649f48eb9854225f7b467015"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/85.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f4fe8936028489c96d9e646dace1b40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2663db53339f4d7ea00eafec63d2c92d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1659083 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d73b336bee14ac681d3757fe6d914c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/520 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a73ba9c505c47de9d622dbb7a8ad70c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2507 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dd67656212f40b7bf617fb7332ee174"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"src_tokenizer = word_tokenizer(eng_hin_dataset, \"en\")\ntgt_tokenizer = word_tokenizer(eng_hin_dataset, \"hi\")","metadata":{"execution":{"iopub.status.busy":"2025-03-03T15:27:10.425492Z","iopub.execute_input":"2025-03-03T15:27:10.425808Z","iopub.status.idle":"2025-03-03T15:27:37.813611Z","shell.execute_reply.started":"2025-03-03T15:27:10.425785Z","shell.execute_reply":"2025-03-03T15:27:37.812889Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_data, val_data = train_test_split(eng_hin_dataset, test_size = 0.2, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2025-03-03T15:27:37.814476Z","iopub.execute_input":"2025-03-03T15:27:37.814706Z","iopub.status.idle":"2025-03-03T15:27:38.474553Z","shell.execute_reply.started":"2025-03-03T15:27:37.814687Z","shell.execute_reply":"2025-03-03T15:27:38.473871Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train_dataset = CustomDataset(train_data, \"en\", \"hi\", src_tokenizer, tgt_tokenizer, 350)\nval_dataset = CustomDataset(val_data, \"en\", \"hi\", src_tokenizer, tgt_tokenizer, 350)","metadata":{"execution":{"iopub.status.busy":"2025-03-03T15:27:38.475343Z","iopub.execute_input":"2025-03-03T15:27:38.475654Z","iopub.status.idle":"2025-03-03T15:27:38.491074Z","shell.execute_reply.started":"2025-03-03T15:27:38.475624Z","shell.execute_reply":"2025-03-03T15:27:38.490216Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"batch_size = 4\ntrain_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\nval_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2025-03-03T15:27:38.491835Z","iopub.execute_input":"2025-03-03T15:27:38.492116Z","iopub.status.idle":"2025-03-03T15:27:38.500157Z","shell.execute_reply.started":"2025-03-03T15:27:38.492083Z","shell.execute_reply":"2025-03-03T15:27:38.499372Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"src_tokenizer.get_vocab_size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T15:27:38.500943Z","iopub.execute_input":"2025-03-03T15:27:38.501204Z","iopub.status.idle":"2025-03-03T15:27:38.522252Z","shell.execute_reply.started":"2025-03-03T15:27:38.501184Z","shell.execute_reply":"2025-03-03T15:27:38.521362Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"30000"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"tgt_tokenizer.get_vocab_size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T15:27:38.523135Z","iopub.execute_input":"2025-03-03T15:27:38.523461Z","iopub.status.idle":"2025-03-03T15:27:38.542934Z","shell.execute_reply.started":"2025-03-03T15:27:38.523431Z","shell.execute_reply":"2025-03-03T15:27:38.542201Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"30000"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# create the model\nmodel = trans_model(src_tokenizer.get_vocab_size(), tgt_tokenizer.get_vocab_size(), 350, 350, d_model=512, num_heads=8, d_ff=2048, num_layers=6, dropout=0.1)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2025-03-03T15:27:38.545549Z","iopub.execute_input":"2025-03-03T15:27:38.545841Z","iopub.status.idle":"2025-03-03T15:27:40.116440Z","shell.execute_reply.started":"2025-03-03T15:27:38.545819Z","shell.execute_reply":"2025-03-03T15:27:40.115473Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Transformer(\n  (encoder): Encoder(\n    (layers): ModuleList(\n      (0-5): 6 x EncoderBlock(\n        (self_attention): MultiHeadAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (w_q): Linear(in_features=512, out_features=512, bias=False)\n          (w_k): Linear(in_features=512, out_features=512, bias=False)\n          (w_v): Linear(in_features=512, out_features=512, bias=False)\n          (w_o): Linear(in_features=512, out_features=512, bias=False)\n        )\n        (feed_forward_network): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (relu): ReLU()\n        )\n        (residual_connection_1): ResidualConnection(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (residual_connection_2): ResidualConnection(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n  (decoder): Decoder(\n    (decoder_blocks): ModuleList(\n      (0-5): 6 x DecoderBlock(\n        (self_attn): MultiHeadAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (w_q): Linear(in_features=512, out_features=512, bias=False)\n          (w_k): Linear(in_features=512, out_features=512, bias=False)\n          (w_v): Linear(in_features=512, out_features=512, bias=False)\n          (w_o): Linear(in_features=512, out_features=512, bias=False)\n        )\n        (cross_attn): MultiHeadAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (w_q): Linear(in_features=512, out_features=512, bias=False)\n          (w_k): Linear(in_features=512, out_features=512, bias=False)\n          (w_v): Linear(in_features=512, out_features=512, bias=False)\n          (w_o): Linear(in_features=512, out_features=512, bias=False)\n        )\n        (feed_forward): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (relu): ReLU()\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n        (residual_connection1): ResidualConnection(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (residual_connection2): ResidualConnection(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (residual_connection3): ResidualConnection(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n  (src_embedding): InputEmbedding(\n    (embedding): Embedding(30000, 512)\n  )\n  (trg_embedding): InputEmbedding(\n    (embedding): Embedding(30000, 512)\n  )\n  (src_postional_encoding): PositionalEmbedding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (trg_postional_encoding): PositionalEmbedding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (linear): LinearLayer(\n    (linear): Linear(in_features=512, out_features=30000, bias=True)\n  )\n)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T15:27:40.117981Z","iopub.execute_input":"2025-03-03T15:27:40.118313Z","iopub.status.idle":"2025-03-03T15:27:40.479838Z","shell.execute_reply.started":"2025-03-03T15:27:40.118278Z","shell.execute_reply":"2025-03-03T15:27:40.479101Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(ignore_index = tgt_tokenizer.token_to_id('[PAD]'), label_smoothing=0.1)\noptimizer = optim.Adam(model.parameters(), lr = 10**-4, eps=1e-9)\nnum_epochs =  20\nmax_seq_len = 350","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T15:27:40.480770Z","iopub.execute_input":"2025-03-03T15:27:40.481076Z","iopub.status.idle":"2025-03-03T15:27:42.622746Z","shell.execute_reply.started":"2025-03-03T15:27:40.481048Z","shell.execute_reply":"2025-03-03T15:27:42.622048Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Decoding the tokens\ndef decode_tokens(tokens, tokenizer):\n    return tokenizer.decode(tokens, skip_special_tokens = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T15:27:42.623416Z","iopub.execute_input":"2025-03-03T15:27:42.623893Z","iopub.status.idle":"2025-03-03T15:27:42.627502Z","shell.execute_reply.started":"2025-03-03T15:27:42.623870Z","shell.execute_reply":"2025-03-03T15:27:42.626584Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"      # out = {\n      #       \"encoder_input\": encoder_input,\n      #       \"decoder_input\": decoder_input,\n      #       \"target\": target,\n      #       \"encoder_mask\" : (encoder_input != self.pad_token_id).unsqueeze(0).unsqueeze(0),\n      #       \"decoder_mask\" : (decoder_input != self.pad_token_id).unsqueeze(0).unsqueeze(0) & cml(decoder_input.size(0)),\n      #       \"src_sen\": src_sen,\n      #       \"tgt_sen\": tgt_sen\n      #   }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T15:27:42.628710Z","iopub.execute_input":"2025-03-03T15:27:42.629057Z","iopub.status.idle":"2025-03-03T15:27:42.651973Z","shell.execute_reply.started":"2025-03-03T15:27:42.629025Z","shell.execute_reply":"2025-03-03T15:27:42.650849Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def training(model, criterion, optimizer, train_loader, tokenizer, epoch, total_epoch):\n    model.train()\n    train_loss = 0\n    total_samples = 0\n\n    # Use tqdm for progress tracking\n    loop = tqdm(train_loader, desc=f\"Training Epoch [{epoch}/{total_epoch}]\", leave=True)\n\n    for idx, data in enumerate(loop):\n        encoder_input = data['encoder_input'].to(device)\n        decoder_input = data['decoder_input'].to(device)\n        target = data['target'].to(device)\n        encoder_mask = data['encoder_mask'].to(device)\n        decoder_mask = data['decoder_mask'].to(device)\n\n        optimizer.zero_grad()  # Reset gradients before backpropagation\n\n        encoder_output = model.encode(encoder_input, encoder_mask)\n        decoder_output = model.decode( decoder_input,encoder_output, encoder_mask, decoder_mask)\n        output = model.linearlayer(decoder_output)\n\n        loss = criterion(output.view(-1, tokenizer.get_vocab_size()), target.view(-1))\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item() * encoder_input.size(0)\n        total_samples += encoder_input.size(0)\n\n        # Update tqdm progress bar with loss\n        loop.set_postfix(train_loss=train_loss / total_samples)\n\n    return train_loss / total_samples\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T15:27:42.653020Z","iopub.execute_input":"2025-03-03T15:27:42.653359Z","iopub.status.idle":"2025-03-03T15:27:42.673508Z","shell.execute_reply.started":"2025-03-03T15:27:42.653326Z","shell.execute_reply":"2025-03-03T15:27:42.672169Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def validation(model, criterion, val_loader, tokenizer, max_seq_len, epoch, total_epoch):\n    model.eval()\n    val_loss = 0\n    total_samples = 0\n\n    with torch.no_grad():\n        loop = tqdm(val_loader, desc=f\"Validation Epoch [{epoch}/{total_epoch}]\", leave=True)\n\n        for idx, data in enumerate(loop):\n            encoder_input = data['encoder_input'].to(device)\n            encoder_mask = data['encoder_mask'].to(device)\n            target = data['target'].to(device)\n\n            batch_size = encoder_input.size(0)\n            assert batch_size == 1, \"Batch size must be 1 for auto-regressive decoding\"\n\n            encoder_output = model.encode(encoder_input, encoder_mask)\n\n            # Convert source tokens to text\n            source_sentence = tokenizer.decode(encoder_input.squeeze(0).tolist())\n\n            # Start decoding with the <sos> token\n            decoder_input = torch.full((batch_size, 1), tokenizer.token_to_id('[SOS]'), dtype=torch.long, device=device)\n            predicted_out = decoder_input  # To store generated sequence\n\n            while True:\n                if decoder_input.size(1) == max_seq_len:\n                    break\n\n                decoder_mask = torch.tril(torch.ones((decoder_input.size(1), decoder_input.size(1)), device=device)).unsqueeze(0)\n                decoder_output = model.decode(decoder_input,encoder_output, encoder_mask, decoder_mask)\n                output = model.linear(decoder_output[:, -1])  # Get last token predictions\n\n                _, next_word = torch.max(output, dim=1)\n                predicted_out = torch.cat([predicted_out, next_word.unsqueeze(1)], dim=1)\n\n                if next_word.item() == tokenizer.token_to_id('[EOS]'):\n                    break\n\n            # Compute loss\n            loss = criterion(output.view(-1, tokenizer.get_vocab_size()), target.view(-1))\n            val_loss += loss.item() * batch_size\n            total_samples += batch_size\n\n            # Convert predicted token IDs to sentence\n            predicted_sentence = tokenizer.decode(predicted_out.squeeze(0).tolist())\n\n            # Convert target tokens to text\n            target_sentence = tokenizer.decode(target.squeeze(0).tolist())\n\n            # Print source, predicted, and target sentences\n            print(\"Source Sentence:  \", source_sentence)\n            print(\"Predicted Sentence:\", predicted_sentence)\n            print(\"Target Sentence:   \", target_sentence)\n\n            # Update tqdm progress bar\n            loop.set_postfix(val_loss=val_loss / total_samples)\n\n    return val_loss / total_samples\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T15:27:42.674610Z","iopub.execute_input":"2025-03-03T15:27:42.674968Z","iopub.status.idle":"2025-03-03T15:27:42.692037Z","shell.execute_reply.started":"2025-03-03T15:27:42.674939Z","shell.execute_reply":"2025-03-03T15:27:42.691232Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# training and validation loop \ndef training_model(model, criterion, optimizer, train_loader, val_loader,src_tokenizer, tgt_tokenizer, max_seq_len, num_epochs):\n    for epoch in range(num_epochs):\n        train_loss = training(model, criterion, optimizer, train_loader, src_tokenizer, epoch, num_epochs)\n        val_loss = validation(model, criterion, val_loader, tgt_tokenizer, max_seq_len, epoch, num_epochs)\n\n        print(f\"Epoch: {epoch+1}, Train Loss: {train_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T15:27:42.692970Z","iopub.execute_input":"2025-03-03T15:27:42.693213Z","iopub.status.idle":"2025-03-03T15:27:42.710895Z","shell.execute_reply.started":"2025-03-03T15:27:42.693193Z","shell.execute_reply":"2025-03-03T15:27:42.709861Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"training_model(model, criterion, optimizer, train_loader, val_loader, src_tokenizer, tgt_tokenizer, max_seq_len, num_epochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T15:27:42.711852Z","iopub.execute_input":"2025-03-03T15:27:42.712229Z"}},"outputs":[{"name":"stderr","text":"Training Epoch [0/20]:   0%|          | 52/331817 [00:08<12:55:10,  7.13it/s, train_loss=9.3] ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}