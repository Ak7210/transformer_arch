{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the important libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformer_model import build_transformer\n",
    "# from datasets import load_dataset #hugging face datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Huggingface datasets and tokenizers\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordLevel\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "# Huggingface datasets and tokenizers\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Custom Dataset\n",
    "    - pupose is to convert the dataset into the map-style dataset\n",
    "    - Tokenize the datasets (source dataset and target dataset)\n",
    "    - And Pad the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cml(size):\n",
    "    # torch.triu returns the upper triangular marix of ones\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal = 1).type(torch.int)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 1., 1., 1.],\n",
       "         [0., 0., 1., 1., 1.],\n",
       "         [0., 0., 0., 1., 1.],\n",
       "         [0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.triu(1,5,5)\n",
    "a = torch.ones((1,5,5))\n",
    "print(a)\n",
    "torch.triu(a, diagonal = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,data, src_len, tgt_len, src_tokenizer, tgt_tokenizer, max_seq_len):\n",
    "        self.data = data\n",
    "        self.src_len = src_len\n",
    "        self.tgt_len = tgt_len\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Create the token_id for special tokens\n",
    "        self.sos_token_id = torch.tensor([tgt_tokenizer.token_to_id('<sos>')], dtype= int64)\n",
    "        self.eos_token_id = torch.tensor([tgt_tokenizer.token_to_id('<eos>')], dtype = int64)\n",
    "        self.pad_token_id = torch.tensor([tgt_tokenizer.token_to_id('<pad>')], dtype = int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the source and target sentence\n",
    "        src_sent = self.data[idx]['translation'][self.src_len]\n",
    "        tgt_sent = self.data[idx]['translation'][self.tgt_len]\n",
    "\n",
    "        # Tokenize the source and target sentence\n",
    "        encoder_sen = self.src_tokenizer.encode(src_sent).ids\n",
    "        decoder_sen = self.tgt_tokenizer.encode(tgt_sent).ids\n",
    "\n",
    "        # Add the special tokens to the soruce and target sentence\n",
    "        src_pad_len = self.max_seq_len - len(encoder_sen) - 2\n",
    "        tgt_pad_len = self.max_seq_len - len(decoder_sen) - 2\n",
    "\n",
    "        # constraint check \n",
    "        if src_pad_len < 0 or tgt_pad_len < 0:\n",
    "            raise ValueError(\"The sentence is too long\")\n",
    "\n",
    "        # use the cat function to concatenate the tensors\n",
    "\n",
    "        encoder_input = torch.cat([self.sos_token_id, torch.tensor(encoder_sen), self.eos_token_id, self.pad_token_id.repeat(src_pad_len)], dim=0) # dim = 0 means concatenate the tokens along with rows\n",
    "        decoder_input = torch.cat([self.sos_token_id, torch.tensor(decoder_sen), self.pad_token_id.repeat(tgt_pad_len)], dim = 0)\n",
    "\n",
    "        # label for the decoder\n",
    "        # add only end token\n",
    "        label = torch.cat([\n",
    "            torch.tensor(decoder_sen, dtypes = int64),\n",
    "            self.eos_token_id,\n",
    "            self.pad_token_id.repeat(tgt_pad_len)\n",
    "        ], dim = 0)\n",
    "\n",
    "        # check the max_seq_len equal\n",
    "        assert encoder_input.size(0) == self.max_seq_len\n",
    "        assert decoder_input.size(0) == self.max_seq_len\n",
    "        assert label.size(0) == self.max_seq_len\n",
    "\n",
    "        # return all the outputs\n",
    "        return {\n",
    "            'encoder_input': encoder_input,\n",
    "            'decoder_input': decoder_input,\n",
    "            'label': label,\n",
    "            'encoder_mask': (encoder_input != self.pad_token_id).unsqueeze(0).unsqueeze(0).int(),\n",
    "            'decoder_mask': (decoder_input != self.pad_token_id).unsqueeze(0).unsqueeze(0).int() & cml(decoder_input.size(0)), #size = (1,1, max_seq_len)\n",
    "            'src_sent': src_sent,\n",
    "            'tgt_sent': tgt_sent\n",
    "            \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(data, leng):\n",
    "    for sen in data:\n",
    "        yield sen['translation'][leng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_train(data, leng):\n",
    "\n",
    "    tokenizer_path = \"tokenizer_{0}.json\".format(leng)\n",
    "    if not os.path.exists(tokenizer_path):\n",
    "        tokenizer1 = Tokenizer(WordLevel(unk_token = \"<unk>\"))\n",
    "        tokenizer1.pre_tokenizer = Whitespace()\n",
    "        trainer1 = WordLevelTrainer(special_tokens=[\"<pad>\", \"<sos>\", \"<eos>\"])\n",
    "        trainer1.train_from_iterator(get_sentences(data, leng), tokenizer1)\n",
    "        tokenizer1.save(str(tokenizer_path))\n",
    "    else: \n",
    "        tokenizer1 = Tokenizer.from_file(str(tokenizer_path))\n",
    "\n",
    "    return tokenizer1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the huggingface datasets\n",
    "eng_hin_dataset = load_datsset('cfilt', 'iitb-english-hindi', split = 'train')\n",
    "src_tokenizer = tokenizer_train(eng_hin_dataset, \"en\")\n",
    "tgt_tokenizer = tokenizer_train(eng_hin_dataset, \"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into 80:20 ratio using sklearn train_test_split fix the seed value\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_data = train_test_split(eng_hin_dataset, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_data, 'en', 'hi', src_tokenizer, tgt_tokenizer, 350) # 350 is the max_seq_len\n",
    "val_dataset = CustomDataset(val_data, 'en', 'hi', src_tokenizer, tgt_tokenizer, 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m(train_dataset, batch_size \u001b[38;5;241m=\u001b[39m batch_size, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 1, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"num_heads\": 8,\n",
    "\"num_epochs\": 20,\n",
    "\"max_seq_len\": 350,\n",
    "\"d_model\": 512,\n",
    "\"d_ff\": 2048,\n",
    "\"dropout\": 0.1,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_transformer(src_vocab_size= len(src_tokenizer.get_vocab()), tgt_vocab_size= len(tgt_tokenizer.get_vocab()),src_seq_len= max_seq_len, tgt_seq_len= max_seq_len, d_model = d_model,num_heads = num_heads , d_ff = d_ff, dropout= dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device on cuda if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# shift it on parallel gpu\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = tgt_tokenizer.token_to_id('<pad>'), label_smoothing=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 10**-4, eps=1e-9)\n",
    "num_epochs =  20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding the tokens\n",
    "def decode_tokens(tokens, tokenizer):\n",
    "    return tokenizer.decode(tokens, skip_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def training(model, criterion, optimizer, train_loader, tokenizer, epoch, total_epoch):\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "#     # use the tqdm for the progress bar\n",
    "#     for idx, data in enumerate(tqdm(train_loader)):\n",
    "#         encoder_input = data['encoder_input'].to(device)\n",
    "#         decoder_input = data['decoder_input'].to(device)\n",
    "#         target = data['label'].to(device)\n",
    "#         encoder_mask = data['encoder_mask'].to(device)\n",
    "#         decoder_mask = data['decoder_mask'].to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "#         decoder_output = model.decode(encoder_output, decoder_input,encoder_mask,  decoder_mask)\n",
    "#         output = model.linear(decoder_output)\n",
    "#         loss = criterion(output.view(-1, len(tokenizer.get_vocab())), target.view(-1))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad(set_to_none = True)\n",
    "#         train_loss += loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def training(model, criterion, optimizer, train_loader, tokenizer, epoch, total_epoch, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Use tqdm for progress tracking\n",
    "    loop = tqdm(train_loader, desc=f\"Training Epoch [{epoch}/{total_epoch}]\", leave=True)\n",
    "\n",
    "    for idx, data in enumerate(loop):\n",
    "        encoder_input = data['encoder_input'].to(device)\n",
    "        decoder_input = data['decoder_input'].to(device)\n",
    "        target = data['label'].to(device)\n",
    "        encoder_mask = data['encoder_mask'].to(device)\n",
    "        decoder_mask = data['decoder_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Reset gradients before backpropagation\n",
    "\n",
    "        encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "        decoder_output = model.decode(encoder_output, decoder_input, encoder_mask, decoder_mask)\n",
    "        output = model.linear(decoder_output)\n",
    "\n",
    "        loss = criterion(output.view(-1, len(tokenizer.get_vocab())), target.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * encoder_input.size(0)\n",
    "        total_samples += encoder_input.size(0)\n",
    "\n",
    "        # Update tqdm progress bar with loss\n",
    "        loop.set_postfix(train_loss=train_loss / total_samples)\n",
    "\n",
    "    return train_loss / total_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def validation(model, criterion, val_loader, tokenizer, max_seq_len epoch, total_epoch):\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for idx, data in enumerate(val_loader):\n",
    "#             encoder_input = data['encoder_input'].to(device)\n",
    "#             encoder_mask = data['encoder_mask'].to(device)\n",
    "#             target = data['label'].to(device)\n",
    "            \n",
    "\n",
    "#             # check the batch size of encoder_input\n",
    "#             assert encoder_input.size(0) == 1, \"at least one sentence should be there\"\n",
    "\n",
    "#             # get the encoder_output\n",
    "#             encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "#             # decoder input is the sos token\n",
    "#             decoder_input = torch.empty(1,1).fill_(tokenizer.token_to_id('<sos>').type_as(encoder_input).to(device))\n",
    "            \n",
    "#             # create a while loop to generate the output\n",
    "#             while True:\n",
    "#                 if decoder_input.size(1) == max_seq_len:\n",
    "#                     break\n",
    "#                 decoder_mask = cml(decoder_input.size(1)).type_as(encoder_mask).to(device)\n",
    "#                 decoder_output = model.decode(encoder_output, decoder_input, encoder_mask, decoder_mask)\n",
    "#                 output = model.linear(decoder_output[:,-1])\n",
    "#                 # get the last token\n",
    "#                 _, next_word = torch.max(output, dim = 1)\n",
    "#                 predicted_out = torch.cat([decoder_input, torch.empty(1,1).fill_(tokenizer.token_to_id(nex_word.item())).to(device)], dim = 1)\n",
    "\n",
    "#                 if next_word.item() == tokenizer.token_to_id('<eos>'):\n",
    "#                     break\n",
    "            \n",
    "#             # get the target sentence\n",
    "#             predicted_sen = predicted_out.squeeze(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, tokenizer, max_seq_len, epoch, total_epoch, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(val_loader, desc=f\"Validation Epoch [{epoch}/{total_epoch}]\", leave=True)\n",
    "\n",
    "        for idx, data in enumerate(loop):\n",
    "            encoder_input = data['encoder_input'].to(device)\n",
    "            encoder_mask = data['encoder_mask'].to(device)\n",
    "            target = data['label'].to(device)\n",
    "\n",
    "            batch_size = encoder_input.size(0)\n",
    "            assert batch_size == 1, \"Batch size must be 1 for auto-regressive decoding\"\n",
    "\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "\n",
    "            # Convert source tokens to text\n",
    "            source_sentence = tokenizer.decode(encoder_input.squeeze(0).tolist())\n",
    "\n",
    "            # Start decoding with the <sos> token\n",
    "            decoder_input = torch.full((batch_size, 1), tokenizer.token_to_id('<sos>'), dtype=torch.long, device=device)\n",
    "            predicted_out = decoder_input  # To store generated sequence\n",
    "\n",
    "            while True:\n",
    "                if decoder_input.size(1) == max_seq_len:\n",
    "                    break\n",
    "\n",
    "                decoder_mask = torch.tril(torch.ones((decoder_input.size(1), decoder_input.size(1)), device=device)).unsqueeze(0)\n",
    "                decoder_output = model.decode(encoder_output, decoder_input, encoder_mask, decoder_mask)\n",
    "                output = model.linear(decoder_output[:, -1])  # Get last token predictions\n",
    "\n",
    "                _, next_word = torch.max(output, dim=1)\n",
    "                predicted_out = torch.cat([predicted_out, next_word.unsqueeze(1)], dim=1)\n",
    "\n",
    "                if next_word.item() == tokenizer.token_to_id('<eos>'):\n",
    "                    break\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output.view(-1, len(tokenizer.get_vocab())), target.view(-1))\n",
    "            val_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "\n",
    "            # Convert predicted token IDs to sentence\n",
    "            predicted_sentence = tokenizer.decode(predicted_out.squeeze(0).tolist())\n",
    "\n",
    "            # Convert target tokens to text\n",
    "            target_sentence = tokenizer.decode(target.squeeze(0).tolist())\n",
    "\n",
    "            # Print source, predicted, and target sentences\n",
    "            print(\"Source Sentence:  \", source_sentence)\n",
    "            print(\"Predicted Sentence:\", predicted_sentence)\n",
    "            print(\"Target Sentence:   \", target_sentence)\n",
    "\n",
    "            # Update tqdm progress bar\n",
    "            loop.set_postfix(val_loss=val_loss / total_samples)\n",
    "\n",
    "    return val_loss / total_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation loop \n",
    "def training_model(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = training(model, criterion, optimizer, train_loader, src_tokenizer, epoch, num_epochs)\n",
    "        val_loss = validation(model, criterion, val_loader, tgt_tokenizer, max_seq_len, epoch, num_epochs)\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}, Train Loss: {train_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
